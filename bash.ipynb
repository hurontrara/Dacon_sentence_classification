{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZnGv3SzFchZ"},"outputs":[],"source":["!pip install transformers==4.21.2\n","!pip install sentencepiece\n","!pip install torch_optimizer\n","!pip install pytorch-lightning\n","!pip install datasets\n","!pip install evaluate\n","!pip install wandb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Colab_Notebooks/NLP/Projects/Dacon/sentence_classification'\n","\n","!wandb login  # df8cdd3f4ff34f32668158f84ae4a1ede7570a92"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Nyxy0PGFihc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e65700b-070f-4609-fc54-6695f85e568c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading tokenizer_config.json: 100% 61.0/61.0 [00:00<00:00, 66.5kB/s]\n","Downloading config.json: 100% 467/467 [00:00<00:00, 479kB/s]\n","Downloading vocab.txt: 100% 257k/257k [00:00<00:00, 765kB/s]\n","|train| = 12405 |valid| = 4136\n","#total_iters = 62025 #warmup_iters = 0\n","Downloading pytorch_model.bin: 100% 431M/431M [00:07<00:00, 59.8MB/s]\n","Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhurontrara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230105_122551-tyc976py\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMeanpool_dropout(1fold)\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification/runs/tyc976py\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  rank_zero_deprecation(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:329: RuntimeWarning: The lr scheduler dict contains the key(s) ['interval'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n","  rank_zero_warn(\n","\n","  | Name      | Type         | Params\n","-------------------------------------------\n","0 | pool      | Pooling      | 0     \n","1 | model     | ElectraModel | 112 M \n","2 | linear1   | Linear       | 3.1 K \n","3 | linear2   | Linear       | 2.3 K \n","4 | linear3   | Linear       | 2.3 K \n","5 | linear4   | Linear       | 1.5 K \n","6 | layernorm | LayerNorm    | 1.5 K \n","7 | crit      | NLLLoss      | 0     \n","8 | softmax   | LogSoftmax   | 0     \n","9 | Dropout   | Dropout      | 0     \n","-------------------------------------------\n","112 M     Trainable params\n","0         Non-trainable params\n","112 M     Total params\n","449.381   Total estimated model params size (MB)\n","valid_loss : 3.4790589809417725  /  valid_score :  0.0 \n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","batch : 1 / loss : 3.694248914718628 / lr : 1.999999994869069e-05 / grad : inf\n","batch : 101 / loss : 3.760117530822754 / lr : 1.999986654477514e-05 / grad : 104083.2109375\n","batch : 201 / loss : 0.11236024647951126 / lr : 1.9999476598269513e-05 / grad : 8484.359375\n","batch : 301 / loss : 1.1832717657089233 / lr : 1.9998830119177733e-05 / grad : 203424.265625\n"]}],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"    # klue/roberta-base \"kykim/electra-kor-base\" klue/roberta-large  lighthouse/mdeberta-v3-base-kor-further  monologg/koelectra-base-v3-discriminator tunib/electra-ko-base   beomi/KcELECTRA-base-v2022\n","\n","!python3 train_cv.py --name Meanpool_dropout --model_fn model_save --train_data_name train_fold --pretrained_model_name monologg/koelectra-base-v3-discriminator --encoder_lr 2e-5 --decoder_lr 2e-5 --batch_size 1 --iteration_per_update 1 --max_length 384 --valid_fold 1 --n_epochs 5 --dropout_p .1 --MeanPooling --cosine --warmup_ratio 0 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"liysZxlhnBU3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"5Eyv4cGtnK8_","outputId":"2837014b-9d5a-4639-d138-833070d0bb81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading tokenizer_config.json: 100% 373/373 [00:00<00:00, 374kB/s]\n","Downloading vocab.txt: 100% 236k/236k [00:00<00:00, 2.13MB/s]\n","Downloading tokenizer.json: 100% 480k/480k [00:00<00:00, 4.23MB/s]\n","Downloading special_tokens_map.json: 100% 169/169 [00:00<00:00, 268kB/s]\n","|train| = 12400 |valid| = 4144\n","#total_iters = 3100 #warmup_iters = 0\n","Downloading config.json: 100% 870/870 [00:00<00:00, 1.23MB/s]\n","Downloading pytorch_model.bin: 100% 436M/436M [00:05<00:00, 78.8MB/s]\n","Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhurontrara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20221224_092719-ibca7jlo\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbigbird(0fold)\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification/runs/ibca7jlo\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  rank_zero_deprecation(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:329: RuntimeWarning: The lr scheduler dict contains the key(s) ['interval'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n","  rank_zero_warn(\n","\n","  | Name      | Type         | Params\n","-------------------------------------------\n","0 | pool      | Pooling      | 0     \n","1 | model     | BigBirdModel | 113 M \n","2 | linear1   | Linear       | 3.1 K \n","3 | linear2   | Linear       | 2.3 K \n","4 | linear3   | Linear       | 2.3 K \n","5 | linear4   | Linear       | 1.5 K \n","6 | layernorm | LayerNorm    | 1.5 K \n","7 | crit      | NLLLoss      | 0     \n","8 | softmax   | LogSoftmax   | 0     \n","9 | Dropout   | Dropout      | 0     \n","-------------------------------------------\n","113 M     Trainable params\n","0         Non-trainable params\n","113 M     Total params\n","455.071   Total estimated model params size (MB)\n","Attention type 'block_sparse' is not possible if sequence_length: 66 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n","valid_loss : 5.2279558181762695  /  valid_score :  0.0 \n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","batch : 1 / loss : 5.36572265625 / lr : 1.9999979459727323e-05 / grad : inf\n","batch : 101 / loss : 1.7518279552459717 / lr : 1.994662228651873e-05 / grad : 48395.73828125\n","batch : 201 / loss : 0.6453157663345337 / lr : 1.9791199308719943e-05 / grad : 48210.390625\n","batch : 301 / loss : 0.5385897159576416 / lr : 1.9535305376404074e-05 / grad : 30247.779296875\n","batch : 401 / loss : 0.8599382638931274 / lr : 1.91815663075965e-05 / grad : 54776.4765625\n","batch : 501 / loss : 0.8677109479904175 / lr : 1.8733611943828626e-05 / grad : 47155.484375\n","batch : 601 / loss : 0.8896529078483582 / lr : 1.819603890305182e-05 / grad : 58427.2421875\n","batch : 701 / loss : 1.3868932723999023 / lr : 1.7574363412117004e-05 / grad : 44182.15625\n","batch : 49 / loss : 1.073567271232605 / lr : 1.7071067811865477e-05\n","batch : 99 / loss : 0.9475651979446411 / lr : 1.7071067811865477e-05\n","batch : 149 / loss : 1.5998486280441284 / lr : 1.7071067811865477e-05\n","batch : 199 / loss : 1.267627239227295 / lr : 1.7071067811865477e-05\n","batch : 249 / loss : 0.987305760383606 / lr : 1.7071067811865477e-05\n","valid_loss : 0.8204097747802734  /  valid_score :  0.7301249660770077 \n","saved\n","epoch 0 train loss 1.1430717706680298\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","batch : 1 / loss : 0.9515724778175354 / lr : 1.70567214170664e-05 / grad : 732693.625\n","batch : 101 / loss : 0.5596808195114136 / lr : 1.6303699134458805e-05 / grad : 318470.84375\n","batch : 201 / loss : 1.0061211585998535 / lr : 1.548599236846381e-05 / grad : 370540.625\n","batch : 301 / loss : 0.5674415230751038 / lr : 1.4611991897034576e-05 / grad : 199629.015625\n","batch : 401 / loss : 0.6395947933197021 / lr : 1.3690666147719578e-05 / grad : 186549.75\n","batch : 501 / loss : 0.16764536499977112 / lr : 1.2731469169459719e-05 / grad : 89067.515625\n","batch : 601 / loss : 0.6027143001556396 / lr : 1.1744243621252847e-05 / grad : 137369.484375\n","batch : 701 / loss : 1.0745735168457031 / lr : 1.0739119773153179e-05 / grad : 267309.0625\n","batch : 49 / loss : 1.1998013257980347 / lr : 1e-05\n","batch : 99 / loss : 0.8359851837158203 / lr : 1e-05\n","batch : 149 / loss : 1.2888786792755127 / lr : 1e-05\n","batch : 199 / loss : 1.3095684051513672 / lr : 1e-05\n","batch : 249 / loss : 0.9711582660675049 / lr : 1e-05\n","valid_loss : 0.792842447757721  /  valid_score :  0.7410415267067865 \n","saved\n","epoch 1 train loss 0.7130749225616455\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","batch : 1 / loss : 0.829642117023468 / lr : 9.979731674176667e-06 / grad : 899242.6875\n","batch : 101 / loss : 0.8823692202568054 / lr : 8.968154522552448e-06 / grad : 441559.71875\n","batch : 201 / loss : 1.016358494758606 / lr : 7.967165501836873e-06 / grad : 214678.421875\n","batch : 301 / loss : 0.5743758082389832 / lr : 6.987036113937045e-06 / grad : 121113.3203125\n","batch : 401 / loss : 0.6949843168258667 / lr : 6.037823812699792e-06 / grad : 174019.265625\n","batch : 501 / loss : 0.6026034951210022 / lr : 5.129268800825281e-06 / grad : 157974.109375\n","batch : 601 / loss : 0.6031856536865234 / lr : 4.270694082206716e-06 / grad : 181860.359375\n","batch : 701 / loss : 0.3395381569862366 / lr : 3.4709097952943483e-06 / grad : 151349.359375\n","batch : 49 / loss : 1.1380610466003418 / lr : 2.9289321881345257e-06\n","batch : 99 / loss : 0.7910430431365967 / lr : 2.9289321881345257e-06\n","batch : 149 / loss : 1.2012114524841309 / lr : 2.9289321881345257e-06\n","batch : 199 / loss : 1.3256593942642212 / lr : 2.9289321881345257e-06\n","batch : 249 / loss : 0.9708331823348999 / lr : 2.9289321881345257e-06\n","valid_loss : 0.7866286039352417  /  valid_score :  0.7427500596501467 \n","saved\n","epoch 2 train loss 0.6710065007209778\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","batch : 1 / loss : 0.5297504663467407 / lr : 2.9146148416676457e-06 / grad : 540042.25\n","batch : 101 / loss : 0.2703937590122223 / lr : 2.2370509970615308e-06 / grad : 177542.65625\n","batch : 201 / loss : 0.6115168333053589 / lr : 1.639145514173992e-06 / grad : 281793.125\n","batch : 301 / loss : 0.3308628797531128 / lr : 1.127033712354848e-06 / grad : 264770.65625\n","batch : 401 / loss : 0.5313108563423157 / lr : 7.059705516887494e-07 / grad : 277557.03125\n","batch : 501 / loss : 0.9740490317344666 / lr : 3.8027670999362665e-07 / grad : 419163.96875\n","batch : 601 / loss : 0.5048927664756775 / lr : 1.532942468194587e-07 / grad : 309563.15625\n","batch : 701 / loss : 0.6946552991867065 / lr : 2.735230939476141e-08 / grad : 476223.375\n","batch : 49 / loss : 1.1589738130569458 / lr : 0.0\n","batch : 99 / loss : 0.7985244989395142 / lr : 0.0\n","batch : 149 / loss : 1.1995115280151367 / lr : 0.0\n","batch : 199 / loss : 1.3098509311676025 / lr : 0.0\n","batch : 249 / loss : 0.9645587205886841 / lr : 0.0\n","valid_loss : 0.7882211208343506  /  valid_score :  0.7431983134559164 \n","saved\n","epoch 3 train loss 0.640213131904602\n","`Trainer.fit` stopped: `max_epochs=4` reached.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▃▃▆▆██\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss █▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▃▃▆▆██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_f1 ▁▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss █▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 3\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.64021\n","\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 3099\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_f1 0.7432\n","\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.78822\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbigbird(0fold)\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification/runs/ibca7jlo\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221224_092719-ibca7jlo/logs\u001b[0m\n","|train| = 12400 |valid| = 4144\n","#total_iters = 3100 #warmup_iters = 0\n","Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhurontrara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20221224_093749-17g30tuz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbigbird(1fold)\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hurontrara/sentence_classification/runs/17g30tuz\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  rank_zero_deprecation(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:329: RuntimeWarning: The lr scheduler dict contains the key(s) ['interval'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n","  rank_zero_warn(\n","\n","  | Name      | Type         | Params\n","-------------------------------------------\n","0 | pool      | Pooling      | 0     \n","1 | model     | BigBirdModel | 113 M \n","2 | linear1   | Linear       | 3.1 K \n","3 | linear2   | Linear       | 2.3 K \n","4 | linear3   | Linear       | 2.3 K \n","5 | linear4   | Linear       | 1.5 K \n","6 | layernorm | LayerNorm    | 1.5 K \n","7 | crit      | NLLLoss      | 0     \n","8 | softmax   | LogSoftmax   | 0     \n","9 | Dropout   | Dropout      | 0     \n","-------------------------------------------\n","113 M     Trainable params\n","0         Non-trainable params\n","113 M     Total params\n","455.071   Total estimated model params size (MB)\n","Attention type 'block_sparse' is not possible if sequence_length: 95 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n","valid_loss : 5.050874710083008  /  valid_score :  0.0 \n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","batch : 1 / loss : 5.357134819030762 / lr : 1.9999979459727323e-05 / grad : inf\n"]}],"source":["!python3 train_cv.py --name bigbird --model_fn model_save --train_data_name train_fold --pretrained_model_name monologg/kobigbird-bert-base --encoder_lr 2e-5 --decoder_lr 2e-5 --batch_size 16 --iteration_per_update 1 --max_length 512 --valid_fold 0 --n_epochs 4 --dropout_p 0 --MultilayerCLSpooling --cosine --warmup_ratio 0\n","!python3 train_cv.py --name bigbird --model_fn model_save --train_data_name train_fold --pretrained_model_name monologg/kobigbird-bert-base --encoder_lr 2e-5 --decoder_lr 2e-5 --batch_size 16 --iteration_per_update 1 --max_length 512 --valid_fold 1 --n_epochs 4 --dropout_p 0 --MultilayerCLSpooling --cosine --warmup_ratio 0\n","!python3 train_cv.py --name bigbird --model_fn model_save --train_data_name train_fold --pretrained_model_name monologg/kobigbird-bert-base --encoder_lr 2e-5 --decoder_lr 2e-5 --batch_size 16 --iteration_per_update 1 --max_length 512 --valid_fold 2 --n_epochs 4 --dropout_p 0 --MultilayerCLSpooling --cosine --warmup_ratio 0\n","!python3 train_cv.py --name bigbird --model_fn model_save --train_data_name train_fold --pretrained_model_name monologg/kobigbird-bert-base --encoder_lr 2e-5 --decoder_lr 2e-5 --batch_size 16 --iteration_per_update 1 --max_length 512 --valid_fold 3 --n_epochs 4 --dropout_p 0 --MultilayerCLSpooling --cosine --warmup_ratio 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-Do1180F5nY"},"outputs":[],"source":["# !python3 make_oof.py --name Meanpool_dropout --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name Multilayer_CLS --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name Multilayer_CLS_smooth --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name Multilayer_Weight_0.1 --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name deberta_base --model_fn model_save --pretrained_model_name  lighthouse/mdeberta-v3-base-kor-further --batch_size 128 --max_length 512\n","# !python3 make_oof.py --name roberta_base --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 128 --max_length 512\n","# !python3 make_oof.py --name roberta_large_basic_0_0.1 --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name roberta_large_CLS_0.1_0.2 --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name roberta_large_Weight_0.2_0  --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name roberta_base_basic_0_0.3 --model_fn model_save --pretrained_model_name  klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name roberta_base_CLS_0_0 --model_fn model_save --pretrained_model_name  klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name roberta_base_Weight_0.1_0.3 --model_fn model_save --pretrained_model_name  klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name tunib_basic_0_0 --model_fn model_save --pretrained_model_name  tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name tunib_CLS_0.1_0.1 --model_fn model_save --pretrained_model_name  tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name tunib_CLS_0.2_0.2 --model_fn model_save --pretrained_model_name  tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name Meanpool_dropout --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name beomi_CLS_0_0.1 --model_fn model_save --pretrained_model_name  beomi/KcELECTRA-base-v2022 --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name beomi_CLS_0.2_0 --model_fn model_save --pretrained_model_name  beomi/KcELECTRA-base-v2022 --batch_size 256 --max_length 512\n","\n","# !python3 make_oof.py --name tunib_attn_0 --model_fn model_save --pretrained_model_name tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name monologg_attn_0 --model_fn model_save --pretrained_model_name monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name kykim_Attn_.1 --model_fn model_save --pretrained_model_name kykim/electra-kor-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name kykim_Attn_0_trans --model_fn model_save --pretrained_model_name kykim/electra-kor-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name roberta_CLS_0 --model_fn model_save --pretrained_model_name klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name deberta_weight_0 --model_fn model_save --pretrained_model_name lighthouse/mdeberta-v3-base-kor-further --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name tunib_weight_0 --model_fn model_save --pretrained_model_name tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name monologg_weight_.1 --model_fn model_save --pretrained_model_name monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name monologg_CLS_.1 --model_fn model_save --pretrained_model_name monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name tunib_CLS_.1 --model_fn model_save --pretrained_model_name tunib/electra-ko-base --batch_size 256 --max_length 512\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaAaJaDbSeOs"},"outputs":[],"source":["# !python3 prediction.py --name Meanpool_dropout --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 prediction.py --name Multilayer_CLS --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 prediction.py --name Multilayer_CLS_smooth --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 prediction.py --name Multilayer_Weight_0.1 --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 prediction.py --name deberta_base --model_fn model_save --pretrained_model_name  lighthouse/mdeberta-v3-base-kor-further --batch_size 256 --max_length 512\n","# !python3 prediction.py --name roberta_base --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 128 --max_length 512\n","# !python3 prediction.py --name roberta_large_basic_0_0.1 --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 256 --max_length 512\n","# !python3 prediction.py --name roberta_large_CLS_0.1_0.2 --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 256 --max_length 512\n","# !python3 prediction.py --name roberta_large_Weight_0.2_0  --model_fn model_save --pretrained_model_name  klue/roberta-large --batch_size 256 --max_length 512\n","# !python3 prediction.py --name roberta_base_basic_0_0.3 --model_fn model_save --pretrained_model_name  klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 prediction.py --name roberta_base_CLS_0_0 --model_fn model_save --pretrained_model_name  klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 prediction.py --name roberta_base_Weight_0.1_0.3 --model_fn model_save --pretrained_model_name  klue/roberta-base --batch_size 256 --max_length 512\n","# !python3 prediction.py --name tunib_basic_0_0 --model_fn model_save --pretrained_model_name  tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 prediction.py --name tunib_CLS_0.1_0.1 --model_fn model_save --pretrained_model_name  tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 prediction.py --name tunib_CLS_0.2_0.2 --model_fn model_save --pretrained_model_name  tunib/electra-ko-base --batch_size 256 --max_length 512\n","# !python3 make_oof.py --name Meanpool_dropout --model_fn model_save --pretrained_model_name  monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","# !python3 prediction.py --name beomi_CLS_0_0.1 --model_fn model_save --pretrained_model_name  beomi/KcELECTRA-base-v2022 --batch_size 256 --max_length 512\n","# !python3 prediction.py --name beomi_CLS_0.2_0 --model_fn model_save --pretrained_model_name  beomi/KcELECTRA-base-v2022 --batch_size 256 --max_length 512\n","\n","!python3 prediction.py --name tunib_attn_0 --model_fn model_save --pretrained_model_name tunib/electra-ko-base --batch_size 256 --max_length 512\n","!python3 prediction.py --name monologg_attn_0 --model_fn model_save --pretrained_model_name monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","!python3 prediction.py --name kykim_Attn_.1 --model_fn model_save --pretrained_model_name kykim/electra-kor-base --batch_size 256 --max_length 512\n","!python3 prediction.py --name kykim_Attn_0_trans --model_fn model_save --pretrained_model_name kykim/electra-kor-base --batch_size 256 --max_length 512\n","!python3 prediction.py --name roberta_CLS_0 --model_fn model_save --pretrained_model_name klue/roberta-base --batch_size 256 --max_length 512\n","!python3 prediction.py --name deberta_weight_0 --model_fn model_save --pretrained_model_name lighthouse/mdeberta-v3-base-kor-further --batch_size 256 --max_length 512\n","!python3 prediction.py --name tunib_weight_0 --model_fn model_save --pretrained_model_name tunib/electra-ko-base --batch_size 256 --max_length 512\n","!python3 prediction.py --name monologg_weight_.1 --model_fn model_save --pretrained_model_name monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","!python3 prediction.py --name monologg_CLS_.1 --model_fn model_save --pretrained_model_name monologg/koelectra-base-v3-discriminator --batch_size 256 --max_length 512\n","!python3 prediction.py --name tunib_CLS_.1 --model_fn model_save --pretrained_model_name tunib/electra-ko-base --batch_size 256 --max_length 512\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPmbXr3iAB2RBj3bIWEiAMp"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}